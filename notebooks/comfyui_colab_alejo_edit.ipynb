{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaaaaaaaaa"
      },
      "source": [
        "Git clone the repo and install the requirements. (ignore the pip errors about protobuf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bbbbbbbbbb",
        "outputId": "ff337e4d-7fdd-452a-c6ef-cd6f883aac78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "/\n",
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive\n",
            "/content/drive/MyDrive/ComfyUI\n",
            "-= Updating ComfyUI =-\n",
            "remote: Enumerating objects: 65, done.\u001b[K\n",
            "remote: Counting objects: 100% (59/59), done.\u001b[K\n",
            "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
            "remote: Total 65 (delta 36), reused 47 (delta 35), pack-reused 6\u001b[K\n",
            "Unpacking objects: 100% (65/65), 42.94 KiB | 7.00 KiB/s, done.\n",
            "From https://github.com/comfyanonymous/ComfyUI\n",
            "   876dadc..91ed281  master     -> origin/master\n",
            "Updating 876dadc..91ed281\n",
            "Fast-forward\n",
            " comfy/sd.py                                 |   7 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " comfy_extras/nodes_canny.py                 | 299 \u001b[32m++++++++++++++++++++++++++++\u001b[m\n",
            " comfy_extras/nodes_model_merging.py         |  22 \u001b[32m++\u001b[m\n",
            " main.py                                     |  36 \u001b[32m++\u001b[m\u001b[31m--\u001b[m\n",
            " nodes.py                                    |  15 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " web/extensions/core/clipspace.js            |   6 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " web/extensions/core/colorPalette.js         |   4 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " web/extensions/core/editAttention.js        |   2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " web/extensions/core/invertMenuScrolling.js  |   2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " web/extensions/core/keybinds.js             |   2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " web/extensions/core/maskeditor.js           |  13 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " web/extensions/core/nodeTemplates.js        |   4 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " web/extensions/core/saveImageExtraOutput.js |   2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " web/extensions/core/slotDefaults.js         |   4 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " web/extensions/core/snapToGrid.js           |   2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " web/extensions/core/uploadImage.js          |   2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " web/extensions/core/widgetInputs.js         |   4 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " web/index.html                              |  10 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " web/scripts/api.js                          |  28 \u001b[32m++\u001b[m\u001b[31m-\u001b[m\n",
            " web/scripts/app.js                          |   4 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " web/scripts/ui.js                           |  39 \u001b[32m++++\u001b[m\n",
            " web/scripts/widgets.js                      |   6 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " web/types/comfy.d.ts                        |   2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " 23 files changed, 454 insertions(+), 61 deletions(-)\n",
            " create mode 100644 comfy_extras/nodes_canny.py\n",
            "-= Install dependencies =-\n",
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu118, https://download.pytorch.org/whl/cu117\n",
            "Collecting xformers!=0.0.18\n",
            "  Downloading xformers-0.0.20-cp310-cp310-manylinux2014_x86_64.whl (109.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.1/109.1 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (2.0.1+cu118)\n",
            "Collecting torchdiffeq (from -r requirements.txt (line 2))\n",
            "  Downloading torchdiffeq-0.2.3-py3-none-any.whl (31 kB)\n",
            "Collecting torchsde (from -r requirements.txt (line 3))\n",
            "  Downloading torchsde-0.2.5-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting einops (from -r requirements.txt (line 4))\n",
            "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers>=4.25.1 (from -r requirements.txt (line 5))\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.0 (from -r requirements.txt (line 6))\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (3.8.4)\n",
            "Collecting accelerate (from -r requirements.txt (line 8))\n",
            "  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (6.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (8.4.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (1.10.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xformers!=0.0.18) (1.22.4)\n",
            "Collecting pyre-extensions==0.0.29 (from xformers!=0.0.18)\n",
            "  Downloading pyre_extensions-0.0.29-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (2.0.0)\n",
            "Collecting typing-inspect (from pyre-extensions==0.0.29->xformers!=0.0.18)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->-r requirements.txt (line 1)) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->-r requirements.txt (line 1)) (16.0.6)\n",
            "Collecting boltons>=20.2.1 (from torchsde->-r requirements.txt (line 3))\n",
            "  Downloading boltons-23.0.0-py2.py3-none-any.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trampoline>=0.1.2 (from torchsde->-r requirements.txt (line 3))\n",
            "  Downloading trampoline-0.1.2-py3-none-any.whl (5.2 kB)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers>=4.25.1->-r requirements.txt (line 5))\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->-r requirements.txt (line 5)) (23.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->-r requirements.txt (line 5)) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->-r requirements.txt (line 5)) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers>=4.25.1->-r requirements.txt (line 5))\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m100.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->-r requirements.txt (line 7)) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->-r requirements.txt (line 7)) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->-r requirements.txt (line 7)) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->-r requirements.txt (line 7)) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->-r requirements.txt (line 7)) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->-r requirements.txt (line 7)) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->-r requirements.txt (line 7)) (1.3.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 8)) (5.9.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers>=4.25.1->-r requirements.txt (line 5)) (2023.6.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->aiohttp->-r requirements.txt (line 7)) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r requirements.txt (line 1)) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.25.1->-r requirements.txt (line 5)) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.25.1->-r requirements.txt (line 5)) (2023.5.7)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->-r requirements.txt (line 1)) (1.3.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect->pyre-extensions==0.0.29->xformers!=0.0.18)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: trampoline, tokenizers, safetensors, boltons, mypy-extensions, einops, typing-inspect, huggingface-hub, transformers, pyre-extensions, xformers, torchsde, torchdiffeq, accelerate\n",
            "Successfully installed accelerate-0.21.0 boltons-23.0.0 einops-0.6.1 huggingface-hub-0.16.4 mypy-extensions-1.0.0 pyre-extensions-0.0.29 safetensors-0.3.1 tokenizers-0.13.3 torchdiffeq-0.2.3 torchsde-0.2.5 trampoline-0.1.2 transformers-4.30.2 typing-inspect-0.9.0 xformers-0.0.20\n"
          ]
        }
      ],
      "source": [
        "#@title Environment Setup\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "OPTIONS = {}\n",
        "\n",
        "USE_GOOGLE_DRIVE = True  #@param {type:\"boolean\"}\n",
        "UPDATE_COMFY_UI = True  #@param {type:\"boolean\"}\n",
        "WORKSPACE = 'ComfyUI'\n",
        "OPTIONS['USE_GOOGLE_DRIVE'] = USE_GOOGLE_DRIVE\n",
        "OPTIONS['UPDATE_COMFY_UI'] = UPDATE_COMFY_UI\n",
        "\n",
        "if OPTIONS['USE_GOOGLE_DRIVE']:\n",
        "    !echo \"Mounting Google Drive...\"\n",
        "    %cd /\n",
        "\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    WORKSPACE = \"/content/drive/MyDrive/ComfyUI\"\n",
        "    %cd /content/drive/MyDrive\n",
        "\n",
        "![ ! -d $WORKSPACE ] && echo -= Initial setup ComfyUI =- && git clone https://github.com/comfyanonymous/ComfyUI\n",
        "%cd $WORKSPACE\n",
        "\n",
        "if OPTIONS['UPDATE_COMFY_UI']:\n",
        "  !echo -= Updating ComfyUI =-\n",
        "  !git pull\n",
        "\n",
        "!echo -= Install dependencies =-\n",
        "!pip install xformers!=0.0.18 -r requirements.txt --extra-index-url https://download.pytorch.org/whl/cu118 --extra-index-url https://download.pytorch.org/whl/cu117"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cccccccccc"
      },
      "source": [
        "Download some models/checkpoints/vae or custom comfyui nodes (uncomment the commands for the ones you want)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dddddddddd",
        "outputId": "57d55bcd-55ba-45ba-a2d2-3e5c3559037b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El modelo ya fue descargado\n",
            "El modelo ya fue descargado\n"
          ]
        }
      ],
      "source": [
        "# Checkpoints\n",
        "# SDXL 0.9\n",
        "import os\n",
        "carpeta = '/content/drive/MyDrive/ComfyUI/models/checkpoints'\n",
        "nombre_archivo_base = 'sd_xl_base_0.9.safetensors'\n",
        "ruta_archivo_base = os.path.join(carpeta, nombre_archivo_base)\n",
        "if os.path.exists(ruta_archivo_base):\n",
        "    print(\"El modelo ya fue descargado\")\n",
        "else:\n",
        "    print(\"Descargando el modelo base\")\n",
        "    !wget -c --header=\"Authorization: Bearer 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\" https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9/resolve/main/sd_xl_base_0.9.safetensors -P ./models/checkpoints/\n",
        "\n",
        "nombre_archivo_refiner = 'sd_xl_refiner_0.9.safetensors'\n",
        "ruta_archivo_refiner = os.path.join(carpeta, nombre_archivo_refiner)\n",
        "if os.path.exists(ruta_archivo_refiner):\n",
        "  print(\"El modelo ya fue descargado\")\n",
        "else:\n",
        "  print(\"Descargando el modelo refiner\")\n",
        "  !wget -c --header=\"Authorization: Bearer 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\" https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9/resolve/main/sd_xl_refiner_0.9.safetensors -P ./models/checkpoints/\n",
        "\n",
        "# SD1.5\n",
        "#!wget -c https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/main/v1-5-pruned-emaonly.ckpt -P ./models/checkpoints/\n",
        "\n",
        "# SD2\n",
        "#!wget -c https://huggingface.co/stabilityai/stable-diffusion-2-1-base/resolve/main/v2-1_512-ema-pruned.safetensors -P ./models/checkpoints/\n",
        "#!wget -c https://huggingface.co/stabilityai/stable-diffusion-2-1/resolve/main/v2-1_768-ema-pruned.safetensors -P ./models/checkpoints/\n",
        "\n",
        "# Some SD1.5 anime style\n",
        "#!wget -c https://huggingface.co/WarriorMama777/OrangeMixs/resolve/main/Models/AbyssOrangeMix2/AbyssOrangeMix2_hard.safetensors -P ./models/checkpoints/\n",
        "#!wget -c https://huggingface.co/WarriorMama777/OrangeMixs/resolve/main/Models/AbyssOrangeMix3/AOM3A1_orangemixs.safetensors -P ./models/checkpoints/\n",
        "#!wget -c https://huggingface.co/WarriorMama777/OrangeMixs/resolve/main/Models/AbyssOrangeMix3/AOM3A3_orangemixs.safetensors -P ./models/checkpoints/\n",
        "#!wget -c https://huggingface.co/Linaqruf/anything-v3.0/resolve/main/anything-v3-fp16-pruned.safetensors -P ./models/checkpoints/\n",
        "\n",
        "# Waifu Diffusion 1.5 (anime style SD2.x 768-v)\n",
        "#!wget -c https://huggingface.co/waifu-diffusion/wd-1-5-beta2/resolve/main/checkpoints/wd-1-5-beta2-fp16.safetensors -P ./models/checkpoints/\n",
        "\n",
        "\n",
        "# unCLIP models\n",
        "#!wget -c https://huggingface.co/comfyanonymous/illuminatiDiffusionV1_v11_unCLIP/resolve/main/illuminatiDiffusionV1_v11-unclip-h-fp16.safetensors -P ./models/checkpoints/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/wd-1.5-beta2_unCLIP/resolve/main/wd-1-5-beta2-aesthetic-unclip-h-fp16.safetensors -P ./models/checkpoints/\n",
        "\n",
        "\n",
        "# VAE\n",
        "#!wget -c https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.safetensors -P ./models/vae/\n",
        "#!wget -c https://huggingface.co/WarriorMama777/OrangeMixs/resolve/main/VAEs/orangemix.vae.pt -P ./models/vae/\n",
        "#!wget -c https://huggingface.co/hakurei/waifu-diffusion-v1-4/resolve/main/vae/kl-f8-anime2.ckpt -P ./models/vae/\n",
        "\n",
        "\n",
        "# Loras\n",
        "#!wget -c https://civitai.com/api/download/models/10350 -O ./models/loras/theovercomer8sContrastFix_sd21768.safetensors #theovercomer8sContrastFix SD2.x 768-v\n",
        "#!wget -c https://civitai.com/api/download/models/10638 -O ./models/loras/theovercomer8sContrastFix_sd15.safetensors #theovercomer8sContrastFix SD1.x\n",
        "\n",
        "\n",
        "# T2I-Adapter\n",
        "#!wget -c https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_depth_sd14v1.pth -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_seg_sd14v1.pth -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_sketch_sd14v1.pth -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_keypose_sd14v1.pth -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_openpose_sd14v1.pth -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_color_sd14v1.pth -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_canny_sd14v1.pth -P ./models/controlnet/\n",
        "\n",
        "# T2I Styles Model\n",
        "#!wget -c https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_style_sd14v1.pth -P ./models/style_models/\n",
        "\n",
        "# CLIPVision model (needed for styles model)\n",
        "#!wget -c https://huggingface.co/openai/clip-vit-large-patch14/resolve/main/pytorch_model.bin -O ./models/clip_vision/clip_vit14.bin\n",
        "\n",
        "\n",
        "# ControlNet\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11e_sd15_ip2p_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11e_sd15_shuffle_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_canny_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11f1p_sd15_depth_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_inpaint_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_lineart_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_mlsd_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_normalbae_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_openpose_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_scribble_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_seg_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_softedge_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15s2_lineart_anime_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11u_sd15_tile_fp16.safetensors -P ./models/controlnet/\n",
        "\n",
        "\n",
        "# Controlnet Preprocessor nodes by Fannovel16\n",
        "#!cd custom_nodes && git clone https://github.com/Fannovel16/comfy_controlnet_preprocessors; cd comfy_controlnet_preprocessors && python install.py\n",
        "\n",
        "\n",
        "# GLIGEN\n",
        "#!wget -c https://huggingface.co/comfyanonymous/GLIGEN_pruned_safetensors/resolve/main/gligen_sd14_textbox_pruned_fp16.safetensors -P ./models/gligen/\n",
        "\n",
        "\n",
        "# ESRGAN upscale model\n",
        "#!wget -c https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth -P ./models/upscale_models/\n",
        "#!wget -c https://huggingface.co/sberbank-ai/Real-ESRGAN/resolve/main/RealESRGAN_x2.pth -P ./models/upscale_models/\n",
        "#!wget -c https://huggingface.co/sberbank-ai/Real-ESRGAN/resolve/main/RealESRGAN_x4.pth -P ./models/upscale_models/\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkkkkkkkkkkkkk"
      },
      "source": [
        "### Run ComfyUI with localtunnel (Recommended Way)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jjjjjjjjjjjjj",
        "outputId": "3caab38e-9a2c-4384-bfa3-7a74d160bac4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h/tools/node/bin/lt -> /tools/node/lib/node_modules/localtunnel/bin/lt.js\n",
            "+ localtunnel@2.0.2\n",
            "added 22 packages from 22 contributors in 2.827s\n",
            "Total VRAM 15102 MB, total RAM 12983 MB\n",
            "Enabling highvram mode because your GPU has more vram than your computer has ram. If you don't want this use: --normalvram\n",
            "xformers version: 0.0.20\n",
            "2023-07-14 14:32:42.446298: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Set vram state to: HIGH_VRAM\n",
            "Device: cuda:0 Tesla T4\n",
            "Using xformers cross attention\n",
            "\n",
            "ComfyUI finished loading, trying to launch localtunnel (if it gets stuck here localtunnel is having issues)\n",
            "\n",
            "The password/enpoint ip for localtunnel is: 34.124.243.64\n",
            "your url is: https://flat-toys-brush.loca.lt\n",
            "got prompt\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 2048 and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 2048 and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 2048 and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 2048 and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 2048 and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 2048 and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 2048 and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 2048 and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 2048 and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 2048 and using 10 heads.\n",
            "v_prediction False\n",
            "adm 2816\n",
            "making attention of type 'vanilla-xformers' with 512 in_channels\n",
            "building MemoryEfficientAttnBlock with 512 in_channels...\n",
            "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
            "making attention of type 'vanilla-xformers' with 512 in_channels\n",
            "building MemoryEfficientAttnBlock with 512 in_channels...\n",
            "missing {'cond_stage_model.clip_g.transformer.text_model.embeddings.position_ids'}\n",
            "left over keys: dict_keys(['denoiser.log_sigmas', 'denoiser.sigmas'])\n",
            "torch.Size([1, 1280]) 1080 1080 0 0 1080 1080\n",
            "torch.Size([1, 1280]) 1080 1080 0 0 1080 1080\n",
            "100% 30/30 [02:09<00:00,  4.31s/it]\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "v_prediction False\n",
            "adm 2560\n",
            "making attention of type 'vanilla-xformers' with 512 in_channels\n",
            "building MemoryEfficientAttnBlock with 512 in_channels...\n",
            "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
            "making attention of type 'vanilla-xformers' with 512 in_channels\n",
            "building MemoryEfficientAttnBlock with 512 in_channels...\n",
            "missing {'cond_stage_model.clip_g.transformer.text_model.embeddings.position_ids'}\n",
            "left over keys: dict_keys(['denoiser.log_sigmas', 'denoiser.sigmas'])\n",
            "torch.Size([1, 1280]) 1080 1080 0 0 6\n",
            "torch.Size([1, 1280]) 1080 1080 0 0 2.5\n",
            "100% 20/20 [00:59<00:00,  2.97s/it]\n",
            "Prompt executed in 864.91 seconds\n",
            "got prompt\n",
            "torch.Size([1, 1280]) 1080 1080 0 0 1080 1080\n",
            "torch.Size([1, 1280]) 1080 1080 0 0 1080 1080\n",
            "100% 30/30 [02:08<00:00,  4.27s/it]\n",
            "torch.Size([1, 1280]) 1080 1080 0 0 6\n",
            "torch.Size([1, 1280]) 1080 1080 0 0 2.5\n",
            "100% 20/20 [00:57<00:00,  2.88s/it]\n",
            "Prompt executed in 202.88 seconds\n",
            "got prompt\n",
            "torch.Size([1, 1280]) 1080 1080 0 0 1080 1080\n",
            "torch.Size([1, 1280]) 1080 1080 0 0 1080 1080\n",
            "100% 30/30 [02:07<00:00,  4.24s/it]\n",
            "torch.Size([1, 1280]) 1080 1080 0 0 6\n",
            "torch.Size([1, 1280]) 1080 1080 0 0 2.5\n",
            "100% 20/20 [00:57<00:00,  2.89s/it]\n",
            "Prompt executed in 201.70 seconds\n",
            "\n",
            "Stopped server\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/ComfyUI/main.py\", line 170, in <module>\n",
            "    cleanup_temp()\n",
            "  File \"/content/drive/MyDrive/ComfyUI/main.py\", line 102, in cleanup_temp\n",
            "    temp_dir = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"temp\")\n",
            "  File \"/usr/lib/python3.10/posixpath.py\", line 396, in realpath\n",
            "    path, ok = _joinrealpath(filename[:0], filename, strict, {})\n",
            "  File \"/usr/lib/python3.10/posixpath.py\", line 431, in _joinrealpath\n",
            "    st = os.lstat(newpath)\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!npm install -g localtunnel\n",
        "\n",
        "import subprocess\n",
        "import threading\n",
        "import time\n",
        "import socket\n",
        "import urllib.request\n",
        "\n",
        "def iframe_thread(port):\n",
        "  while True:\n",
        "      time.sleep(0.5)\n",
        "      sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "      result = sock.connect_ex(('127.0.0.1', port))\n",
        "      if result == 0:\n",
        "        break\n",
        "      sock.close()\n",
        "  print(\"\\nComfyUI finished loading, trying to launch localtunnel (if it gets stuck here localtunnel is having issues)\\n\")\n",
        "\n",
        "  print(\"The password/enpoint ip for localtunnel is:\", urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\"\\n\"))\n",
        "  p = subprocess.Popen([\"lt\", \"--port\", \"{}\".format(port)], stdout=subprocess.PIPE)\n",
        "  for line in p.stdout:\n",
        "    print(line.decode(), end='')\n",
        "\n",
        "\n",
        "threading.Thread(target=iframe_thread, daemon=True, args=(8188,)).start()\n",
        "\n",
        "!python main.py --dont-print-server"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gggggggggg"
      },
      "source": [
        "### Run ComfyUI with colab iframe (use only in case the previous way with localtunnel doesn't work)\n",
        "\n",
        "You should see the ui appear in an iframe. If you get a 403 error, it's your firefox settings or an extension that's messing things up.\n",
        "\n",
        "If you want to open it in another window use the link.\n",
        "\n",
        "Note that some UI features like live image previews won't work because the colab iframe blocks websockets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhhhhhhhhh"
      },
      "outputs": [],
      "source": [
        "import threading\n",
        "import time\n",
        "import socket\n",
        "def iframe_thread(port):\n",
        "  while True:\n",
        "      time.sleep(0.5)\n",
        "      sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "      result = sock.connect_ex(('127.0.0.1', port))\n",
        "      if result == 0:\n",
        "        break\n",
        "      sock.close()\n",
        "  from google.colab import output\n",
        "  output.serve_kernel_port_as_iframe(port, height=1024)\n",
        "  print(\"to open it in a window you can open this link here:\")\n",
        "  output.serve_kernel_port_as_window(port)\n",
        "\n",
        "threading.Thread(target=iframe_thread, daemon=True, args=(8188,)).start()\n",
        "\n",
        "!python main.py --dont-print-server"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}